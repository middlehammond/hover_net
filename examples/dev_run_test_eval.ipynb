{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess\n",
    "import glob\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "command_template = \"python run_infer.py \\\n",
    "--gpu='0' \\\n",
    "--nr_types={} \\\n",
    "--type_info_path=type_info.json \\\n",
    "--batch_size=8 \\\n",
    "--model_mode={} \\\n",
    "--model_path={} \\\n",
    "--nr_inference_workers=8 \\\n",
    "--nr_post_proc_workers=12 \\\n",
    "tile \\\n",
    "--input_dir=dataset/sample_tiles/imgs/ \\\n",
    "--output_dir=dataset/sample_tiles/pred/ \\\n",
    "--mem_usage=0.1 \\\n",
    "--draw_dot \\\n",
    "--save_qupath\"\n",
    "\n",
    "info = {\n",
    "    \"pannuke\": [\"6\", \"fast\", \"pretrained/hovernet_fast_pannuke_type_tf2pytorch.tar\"],\n",
    "    \"monusac\": [\"5\", \"fast\", \"pretrained/hovernet_fast_monusac_type_tf2pytorch.tar\"],\n",
    "    \"consep_type\": [\"5\", \"original\", \"pretrained/hovernet_original_consep_type_tf2pytorch.tar\"],\n",
    "    \"consep_notype\": [\"0\", \"original\", \"pretrained/hovernet_original_consep_notype_tf2pytorch.tar\"],\n",
    "    \"cpm17\": [\"0\", \"original\", \"pretrained/hovernet_original_cpm17_notype_tf2pytorch.tar\"],\n",
    "    \"kumar\": [\"0\", \"original\", \"pretrained/hovernet_original_kumar_notype_tf2pytorch.tar\"]\n",
    "}\n",
    "\n",
    "info = {\n",
    "    # \"kumar\": [\"0\", \"original\", \"pretrained/hovernet_original_kumar_notype_tf2pytorch.tar\"],\n",
    "    # \"cutom\": [\"0\", \"original\", \"logs/00/net_epoch=10.tar\"],\n",
    "    \"kumar2\": [\"0\", \"original\", \"pretrained/kumar_retrain.tar\"]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dirs = [r\"C:\\Users\\toom\\cuhk-proj\\hover_net\\dataset\\sample_tiles\\pred\\overlay\" ,\n",
    "                r\"C:\\Users\\toom\\cuhk-proj\\hover_net\\dataset\\sample_tiles\\pred\\mat\"]\n",
    "sav_dir = r\"C:\\Users\\toom\\Desktop\\test_kumar\"   # Path to the new dir\n",
    "\n",
    "for dataset, v in info.items():\n",
    "    # Format string\n",
    "    line = command_template.format(*v)\n",
    "\n",
    "    # Run command and wait\n",
    "    process = subprocess.Popen(['powershell.exe', 'conda activate hovernet; ' + line], \\\n",
    "                                stdout=subprocess.PIPE, \\\n",
    "                                cwd=\"C:/Users/toom/cuhk-proj/hover_net\")\n",
    "    process.wait()\n",
    "\n",
    "    if process.returncode != 0:\n",
    "        print(dataset)\n",
    "        continue\n",
    "\n",
    "    # Move files to other dir\n",
    "    for output_dir in output_dirs:\n",
    "        files = glob.glob(os.path.join(output_dir, \"*\"))\n",
    "        curr_sav_dir = os.path.join(sav_dir, dataset, os.path.basename(output_dir))\n",
    "        if not os.path.exists(curr_sav_dir):\n",
    "            os.makedirs(curr_sav_dir)\n",
    "        for i in files:\n",
    "            shutil.move(i, os.path.join(curr_sav_dir, os.path.basename(i)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.utils.data as data\n",
    "from importlib import import_module\n",
    "from run_utils.utils import convert_pytorch_checkpoint\n",
    "from models.hovernet.utils import crop_to_shape, dice_loss, mse_loss, msge_loss, xentropy_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_args =  {\n",
    "                'nr_types'   : None,\n",
    "                'mode'       : \"original\",\n",
    "            }\n",
    "        \n",
    "model_path = r\"C:\\Users\\toom\\cuhk-proj\\hover_net\\pretrained\\hovernet_original_kumar_notype_tf2pytorch.tar\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_desc = import_module(\"models.hovernet.net_desc\")\n",
    "model_creator = getattr(model_desc, \"create_model\")\n",
    "\n",
    "net = model_creator(**model_args)\n",
    "saved_state_dict = torch.load(model_path)[\"desc\"]\n",
    "saved_state_dict = convert_pytorch_checkpoint(saved_state_dict)\n",
    "\n",
    "net.load_state_dict(saved_state_dict, strict=True)\n",
    "net = torch.nn.DataParallel(net)\n",
    "net = net.to(\"cuda\")\n",
    "\n",
    "module_lib = import_module(\"models.hovernet.run_desc\")\n",
    "run_step = getattr(module_lib, \"infer_step\")\n",
    "what = lambda input_batch: run_step(input_batch, net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "mat1 = np.load(r\"C:\\Users\\toom\\cuhk-proj\\hover_net\\dataset\\training_data\\kumar\\kumar\\train\\540x540_164x164\\TCGA-18-5592-01Z-00-DX1_000.npy\")\n",
    "mat1 = mat1.reshape((1, 540, 540, 4))[:, :, :, :3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.io as sio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = glob.glob(r\"C:\\Users\\toom\\Desktop\\data\\kumar\\test_diff\\Labels/*\")\n",
    "names = [os.path.basename(i)[:-4] for i in names]\n",
    "gt_dir = r\"C:\\Users\\toom\\Desktop\\data\\kumar\\test_diff\\Labels\"\n",
    "k_dir = r\"C:\\Users\\toom\\Desktop\\test_kumar\\kumar\\mat\"\n",
    "k2_dir = r\"C:\\Users\\toom\\Desktop\\test_kumar\\kumar2\\mat\"\n",
    "c_dir = r\"C:\\Users\\toom\\Desktop\\test_kumar\\custom\\mat\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "gt = np.array([sio.loadmat(os.path.join(gt_dir, i + \".mat\"))[\"inst_map\"] for i in names])\n",
    "k = np.array([sio.loadmat(os.path.join(k_dir, i + \".mat\"))[\"inst_map\"] for i in names])\n",
    "k2 = np.array([sio.loadmat(os.path.join(k2_dir, i + \".mat\"))[\"inst_map\"] for i in names])\n",
    "c = np.array([sio.loadmat(os.path.join(c_dir, i + \".mat\"))[\"inst_map\"] for i in names])\n",
    "\n",
    "gt[gt > 1] = 1\n",
    "k[k > 1] = 1\n",
    "k2[k2 > 1] = 1\n",
    "c[c > 1] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.)\n",
      "tensor(0.1562)\n",
      "tensor(0.1820)\n",
      "tensor(0.2027)\n"
     ]
    }
   ],
   "source": [
    "print(dice_loss(torch.from_numpy(gt), torch.from_numpy(gt)))\n",
    "print(dice_loss(torch.from_numpy(gt), torch.from_numpy(k)))\n",
    "print(dice_loss(torch.from_numpy(gt), torch.from_numpy(k2)))\n",
    "print(dice_loss(torch.from_numpy(gt), torch.from_numpy(c)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6.12 ('hovernet')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "dddd801360a56fad60e02e43b98fdb255626f37b9835d3f7c403e388af40b10b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
